use crate::datastore::{Error, RowExt, Transaction, check_single_row_mutation};
use janus_core::time::Clock;
use janus_messages::{ReportError, TaskId};
use serde::{Deserialize, Serialize};
use tracing::Level;

/// Per-task counts of uploaded reports and upload attempts.
#[derive(Debug, Clone, Copy, Default, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
pub struct TaskUploadCounter {
    /// Reports that fell into a time interval that had already been collected.
    pub(crate) interval_collected: u64,
    /// Reports that could not be decoded.
    pub(crate) report_decode_failure: u64,
    /// Reports that could not be decrypted.
    pub(crate) report_decrypt_failure: u64,
    /// Reports that contained a timestamp too far in the past.
    pub(crate) report_expired: u64,
    /// Reports that were encrypted with an old or unknown HPKE key.
    pub(crate) report_outdated_key: u64,
    /// Reports that were successfully uploaded.
    pub(crate) report_success: u64,
    /// Reports that contain a timestamp too far in the future.
    pub(crate) report_too_early: u64,
    /// Reports that were submitted to the task before the task's start time.
    pub(crate) task_not_started: u64,
    /// Reports that were submitted to the task after the task's end time.
    pub(crate) task_ended: u64,
}

impl TaskUploadCounter {
    #[allow(clippy::too_many_arguments)]
    #[cfg(feature = "test-util")]
    pub fn new_with_values(
        interval_collected: u64,
        report_decode_failure: u64,
        report_decrypt_failure: u64,
        report_expired: u64,
        report_outdated_key: u64,
        report_success: u64,
        report_too_early: u64,
        task_not_started: u64,
        task_ended: u64,
    ) -> Self {
        Self {
            interval_collected,
            report_decode_failure,
            report_decrypt_failure,
            report_expired,
            report_outdated_key,
            report_success,
            report_too_early,
            task_not_started,
            task_ended,
        }
    }

    /// Load counters for the specified task from the datastore and construct a
    /// [`TaskUploadCounter`]. This is aggregated across all shards. Returns `None` if the task
    /// doesn't exist.
    #[tracing::instrument(skip(tx), err(level = Level::DEBUG))]
    pub async fn load<'a, C: Clock>(
        tx: &Transaction<'a, C>,
        task_id: &TaskId,
    ) -> Result<Option<Self>, Error> {
        let stmt = tx
            .prepare_cached(
                "-- get_task_upload_counter()
SELECT
    tasks.id,
    COALESCE(SUM(interval_collected)::BIGINT, 0) AS interval_collected,
    COALESCE(SUM(report_decode_failure)::BIGINT, 0) AS report_decode_failure,
    COALESCE(SUM(report_decrypt_failure)::BIGINT, 0) AS report_decrypt_failure,
    COALESCE(SUM(report_expired)::BIGINT, 0) AS report_expired,
    COALESCE(SUM(report_outdated_key)::BIGINT, 0) AS report_outdated_key,
    COALESCE(SUM(report_success)::BIGINT, 0) AS report_success,
    COALESCE(SUM(report_too_early)::BIGINT, 0) AS report_too_early,
    COALESCE(SUM(task_not_started)::BIGINT, 0) AS task_not_started,
    COALESCE(SUM(task_ended)::BIGINT, 0) AS task_ended
FROM task_upload_counters
RIGHT JOIN tasks on tasks.id = task_upload_counters.task_id
WHERE tasks.task_id = $1
GROUP BY tasks.id",
            )
            .await?;

        tx.query_opt(&stmt, &[task_id.as_ref()])
            .await?
            .map(|row| {
                Ok(Self {
                    interval_collected: row.get_bigint_and_convert("interval_collected")?,
                    report_decode_failure: row.get_bigint_and_convert("report_decode_failure")?,
                    report_decrypt_failure: row.get_bigint_and_convert("report_decrypt_failure")?,
                    report_expired: row.get_bigint_and_convert("report_expired")?,
                    report_outdated_key: row.get_bigint_and_convert("report_outdated_key")?,
                    report_success: row.get_bigint_and_convert("report_success")?,
                    report_too_early: row.get_bigint_and_convert("report_too_early")?,
                    task_not_started: row.get_bigint_and_convert("task_not_started")?,
                    task_ended: row.get_bigint_and_convert("task_ended")?,
                })
            })
            .transpose()
    }

    /// Add the values in this counter to the counts persisted for the given [`TaskId`]. This is
    /// sharded, requiring an `ord` parameter to determine which shard to add to. `ord` should be
    /// randomly generated by the caller.
    #[tracing::instrument(skip(tx), err(level = Level::DEBUG))]
    pub async fn flush<'a, C: Clock>(
        self,
        task_id: &TaskId,
        tx: &Transaction<'a, C>,
        ord: u64,
    ) -> Result<(), Error> {
        let stmt = "-- increment_task_upload_counter()
INSERT INTO task_upload_counters (
    task_id, ord, interval_collected, report_decode_failure,
    report_decrypt_failure, report_expired, report_outdated_key, report_success, report_too_early,
    task_not_started, task_ended
)
VALUES ((SELECT id FROM tasks WHERE task_id = $1), $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
ON CONFLICT (task_id, ord) DO UPDATE SET
    interval_collected = task_upload_counters.interval_collected + $3,
    report_decode_failure = task_upload_counters.report_decode_failure + $4,
    report_decrypt_failure = task_upload_counters.report_decrypt_failure + $5,
    report_expired = task_upload_counters.report_expired + $6,
    report_outdated_key = task_upload_counters.report_outdated_key + $7,
    report_success = task_upload_counters.report_success + $8,
    report_too_early = task_upload_counters.report_too_early + $9,
    task_not_started = task_upload_counters.task_not_started + $10,
    task_ended = task_upload_counters.task_ended + $11";

        let stmt = tx.prepare_cached(stmt).await?;
        check_single_row_mutation(
            tx.execute(
                &stmt,
                &[
                    task_id.as_ref(),
                    &i64::try_from(ord)?,
                    &i64::try_from(self.interval_collected)?,
                    &i64::try_from(self.report_decode_failure)?,
                    &i64::try_from(self.report_decrypt_failure)?,
                    &i64::try_from(self.report_expired)?,
                    &i64::try_from(self.report_outdated_key)?,
                    &i64::try_from(self.report_success)?,
                    &i64::try_from(self.report_too_early)?,
                    &i64::try_from(self.task_not_started)?,
                    &i64::try_from(self.task_ended)?,
                ],
            )
            .await?,
        )
    }

    pub fn increment_interval_collected(&mut self) {
        self.interval_collected += 1
    }

    pub fn increment_report_decode_failure(&mut self) {
        self.report_decode_failure += 1
    }

    pub fn increment_report_decrypt_failure(&mut self) {
        self.report_decrypt_failure += 1
    }

    pub fn increment_report_expired(&mut self) {
        self.report_expired += 1
    }

    pub fn increment_report_outdated_key(&mut self) {
        self.report_outdated_key += 1
    }

    pub fn increment_report_success(&mut self) {
        self.report_success += 1
    }

    pub fn increment_report_too_early(&mut self) {
        self.report_too_early += 1
    }

    pub fn increment_task_not_started(&mut self) {
        self.task_not_started += 1
    }

    pub fn increment_task_ended(&mut self) {
        self.task_ended += 1
    }

    pub fn interval_collected(&self) -> u64 {
        self.interval_collected
    }

    pub fn report_decode_failure(&self) -> u64 {
        self.report_decode_failure
    }

    pub fn report_decrypt_failure(&self) -> u64 {
        self.report_decrypt_failure
    }

    pub fn report_expired(&self) -> u64 {
        self.report_expired
    }

    pub fn report_outdated_key(&self) -> u64 {
        self.report_outdated_key
    }

    pub fn report_success(&self) -> u64 {
        self.report_success
    }

    pub fn report_too_early(&self) -> u64 {
        self.report_too_early
    }

    pub fn task_not_started(&self) -> u64 {
        self.task_not_started
    }

    pub fn task_ended(&self) -> u64 {
        self.task_ended
    }
}

/// Per-task counts of aggregated reports.
///
/// The intended scope of this structure is a single operation, e.g., a single step of a single
/// aggregation job. What that means is:
///   - The counter values in this structure will not represent the current totals for the task, but
///     rather just the contribution that the particular operation is making to the task total;
///   - Callers must flush the counter values to the datastore by calling
///     `Transaction::increment_task_aggregation_counter`. Callers must avoid double counting: any
///     operation's counters should only be flushed to datastore once.
#[derive(Debug, Clone, Copy, Default, PartialEq, Eq, Serialize, Deserialize)]
pub struct TaskAggregationCounter {
    /// The number of successfully-aggregated reports.
    pub success: u64,

    /// The number of reports rejected due to duplicate extensions.
    pub duplicate_extension: u64,
    /// The number of reports rejected due to failure to encode the public share.
    pub public_share_encode_failure: u64,
    /// The number of reports rejected due to the batch being collected.
    pub batch_collected: u64,
    /// The number of reports rejected due to the report replay.
    pub report_replayed: u64,
    /// The number of reports rejected due to the leader dropping the report.
    pub report_dropped: u64,
    /// The number of reports rejected due to unknown HPKE config ID.
    pub hpke_unknown_config_id: u64,
    /// The number of reports rejected due to HPKE decryption failure.
    pub hpke_decrypt_failure: u64,
    /// The number of reports rejected due to VDAF preparation error.
    pub vdaf_prep_error: u64,
    /// The number of reports rejected due to the task not having started yet.
    pub task_not_started: u64,
    /// The number of reports rejected due to task expiration.
    pub task_expired: u64,
    /// The number of reports rejected due to an invalid message.
    pub invalid_message: u64,
    /// The number of reports rejected due to a report arriving too early.
    pub report_too_early: u64,

    /// The number of reports rejected by the helper due to the batch being collected.
    pub helper_batch_collected: u64,
    /// The number of reports rejected by the helper due to the report replay.
    pub helper_report_replayed: u64,
    /// The number of reports rejected by the helper due to the leader dropping the report.
    pub helper_report_dropped: u64,
    /// The number of reports rejected by the helper due to unknown HPKE config ID.
    pub helper_hpke_unknown_config_id: u64,
    /// The number of reports rejected by the helper due to HPKE decryption failure.
    pub helper_hpke_decrypt_failure: u64,
    /// The number of reports rejected by the helper due to VDAF preparation error.
    pub helper_vdaf_prep_error: u64,
    /// The number of reports rejected by the helper due to the task not having started yet.
    pub helper_task_not_started: u64,
    /// The number of reports rejected by the helper due to task expiration.
    pub helper_task_expired: u64,
    /// The number of reports rejected by the helper due to an invalid message.
    pub helper_invalid_message: u64,
    /// The number of reports rejected by the helper due to a report arriving too early.
    pub helper_report_too_early: u64,
}

impl TaskAggregationCounter {
    /// Load counters for the specified task from the datastore and construct a
    /// [`TaskAggregationCounter`]. This is aggregated across all shards. Returns `None` if the task
    /// doesn't exist.
    #[tracing::instrument(skip(tx), err(level = Level::DEBUG))]
    pub async fn load<'a, C: Clock>(
        tx: &Transaction<'a, C>,
        task_id: &TaskId,
    ) -> Result<Option<Self>, Error> {
        let task_info = match tx.task_info_for(task_id).await? {
            Some(task_info) => task_info,
            None => return Ok(None),
        };

        let stmt = tx
            .prepare_cached(
                "-- get_task_aggregation_counter()
SELECT
    COALESCE(SUM(success)::BIGINT, 0) AS success,
    COALESCE(SUM(duplicate_extension)::BIGINT, 0) as duplicate_extension,
    COALESCE(SUM(public_share_encode_failure)::BIGINT, 0) as public_share_encode_failure,
    COALESCE(SUM(batch_collected)::BIGINT, 0) AS batch_collected,
    COALESCE(SUM(report_replayed)::BIGINT, 0) AS report_replayed,
    COALESCE(SUM(report_dropped)::BIGINT, 0) AS report_dropped,
    COALESCE(SUM(hpke_unknown_config_id)::BIGINT, 0) AS hpke_unknown_config_id,
    COALESCE(SUM(hpke_decrypt_failure)::BIGINT, 0) AS hpke_decrypt_failure,
    COALESCE(SUM(vdaf_prep_error)::BIGINT, 0) AS vdaf_prep_error,
    COALESCE(SUM(task_not_started)::BIGINT, 0) AS task_not_started,
    COALESCE(SUM(task_expired)::BIGINT, 0) AS task_expired,
    COALESCE(SUM(invalid_message)::BIGINT, 0) AS invalid_message,
    COALESCE(SUM(report_too_early)::BIGINT, 0) AS report_too_early,
    COALESCE(SUM(helper_batch_collected)::BIGINT, 0) AS helper_batch_collected,
    COALESCE(SUM(helper_report_replayed)::BIGINT, 0) AS helper_report_replayed,
    COALESCE(SUM(helper_report_dropped)::BIGINT, 0) AS helper_report_dropped,
    COALESCE(SUM(helper_hpke_unknown_config_id)::BIGINT, 0) AS helper_hpke_unknown_config_id,
    COALESCE(SUM(helper_hpke_decrypt_failure)::BIGINT, 0) AS helper_hpke_decrypt_failure,
    COALESCE(SUM(helper_vdaf_prep_error)::BIGINT, 0) AS helper_vdaf_prep_error,
    COALESCE(SUM(helper_task_not_started)::BIGINT, 0) AS helper_task_not_started,
    COALESCE(SUM(helper_task_expired)::BIGINT, 0) AS helper_task_expired,
    COALESCE(SUM(helper_invalid_message)::BIGINT, 0) AS helper_invalid_message,
    COALESCE(SUM(helper_report_too_early)::BIGINT, 0) AS helper_report_too_early
FROM task_aggregation_counters
WHERE task_id = $1",
            )
            .await?;

        tx.query_opt(&stmt, &[/* task_id */ &task_info.pkey])
            .await?
            .map(|row| {
                Ok(Self {
                    success: row.get_bigint_and_convert("success")?,
                    duplicate_extension: row.get_bigint_and_convert("duplicate_extension")?,
                    public_share_encode_failure: row
                        .get_bigint_and_convert("public_share_encode_failure")?,
                    batch_collected: row.get_bigint_and_convert("batch_collected")?,
                    report_replayed: row.get_bigint_and_convert("report_replayed")?,
                    report_dropped: row.get_bigint_and_convert("report_dropped")?,
                    hpke_unknown_config_id: row.get_bigint_and_convert("hpke_unknown_config_id")?,
                    hpke_decrypt_failure: row.get_bigint_and_convert("hpke_decrypt_failure")?,
                    vdaf_prep_error: row.get_bigint_and_convert("vdaf_prep_error")?,
                    task_not_started: row.get_bigint_and_convert("task_not_started")?,
                    task_expired: row.get_bigint_and_convert("task_expired")?,
                    invalid_message: row.get_bigint_and_convert("invalid_message")?,
                    report_too_early: row.get_bigint_and_convert("report_too_early")?,
                    helper_batch_collected: row.get_bigint_and_convert("helper_batch_collected")?,
                    helper_report_replayed: row.get_bigint_and_convert("helper_report_replayed")?,
                    helper_report_dropped: row.get_bigint_and_convert("helper_report_dropped")?,
                    helper_hpke_unknown_config_id: row
                        .get_bigint_and_convert("helper_hpke_unknown_config_id")?,
                    helper_hpke_decrypt_failure: row
                        .get_bigint_and_convert("helper_hpke_decrypt_failure")?,
                    helper_vdaf_prep_error: row.get_bigint_and_convert("helper_vdaf_prep_error")?,
                    helper_task_not_started: row
                        .get_bigint_and_convert("helper_task_not_started")?,
                    helper_task_expired: row.get_bigint_and_convert("helper_task_expired")?,
                    helper_invalid_message: row.get_bigint_and_convert("helper_invalid_message")?,
                    helper_report_too_early: row
                        .get_bigint_and_convert("helper_report_too_early")?,
                })
            })
            .transpose()
    }

    /// Add the values in this counter to the counts persisted for the given [`TaskId`]. This is
    /// sharded, requiring an `ord` parameter to determine which shard to add to. `ord` should be
    /// randomly generated by the caller.
    #[tracing::instrument(skip(tx), err(level = Level::DEBUG))]
    pub async fn flush<'a, C: Clock>(
        self,
        task_id: &TaskId,
        tx: &Transaction<'a, C>,
        ord: u64,
    ) -> Result<(), Error> {
        let task_info = match tx.task_info_for(task_id).await? {
            Some(task_info) => task_info,
            None => return Err(Error::MutationTargetNotFound),
        };

        let stmt = tx
            .prepare_cached(
                "-- increment_task_aggregation_counter()
INSERT INTO task_aggregation_counters (task_id, ord, success, duplicate_extension,
public_share_encode_failure, batch_collected, report_replayed, report_dropped,
hpke_unknown_config_id, hpke_decrypt_failure, vdaf_prep_error, task_not_started, task_expired,
invalid_message, report_too_early, helper_batch_collected, helper_report_replayed,
helper_report_dropped, helper_hpke_unknown_config_id, helper_hpke_decrypt_failure,
helper_vdaf_prep_error, helper_task_not_started, helper_task_expired, helper_invalid_message,
helper_report_too_early)
VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18, $19, $20,
    $21, $22, $23, $24, $25)
ON CONFLICT (task_id, ord) DO UPDATE SET
    success = task_aggregation_counters.success + $3,
    duplicate_extension = task_aggregation_counters.duplicate_extension + $4,
    public_share_encode_failure = task_aggregation_counters.public_share_encode_failure + $5,
    batch_collected = task_aggregation_counters.batch_collected + $6,
    report_replayed = task_aggregation_counters.report_replayed + $7,
    report_dropped = task_aggregation_counters.report_dropped + $8,
    hpke_unknown_config_id = task_aggregation_counters.hpke_unknown_config_id + $9,
    hpke_decrypt_failure = task_aggregation_counters.hpke_decrypt_failure + $10,
    vdaf_prep_error = task_aggregation_counters.vdaf_prep_error + $11,
    task_not_started = task_aggregation_counters.task_not_started + $12,
    task_expired = task_aggregation_counters.task_expired + $13,
    invalid_message = task_aggregation_counters.invalid_message + $14,
    report_too_early = task_aggregation_counters.report_too_early + $15,
    helper_batch_collected = task_aggregation_counters.helper_batch_collected + $16,
    helper_report_replayed = task_aggregation_counters.helper_report_replayed + $17,
    helper_report_dropped = task_aggregation_counters.helper_report_dropped + $18,
    helper_hpke_unknown_config_id = task_aggregation_counters.helper_hpke_unknown_config_id + $19,
    helper_hpke_decrypt_failure = task_aggregation_counters.helper_hpke_decrypt_failure + $20,
    helper_vdaf_prep_error = task_aggregation_counters.helper_vdaf_prep_error + $21,
    helper_task_not_started = task_aggregation_counters.helper_task_not_started + $22,
    helper_task_expired = task_aggregation_counters.helper_task_expired + $23,
    helper_invalid_message = task_aggregation_counters.helper_invalid_message + $24,
    helper_report_too_early = task_aggregation_counters.helper_report_too_early + $25",
            )
            .await?;

        check_single_row_mutation(
            tx.execute(
                &stmt,
                &[
                    /* task_id */ &task_info.pkey,
                    /* ord */ &i64::try_from(ord)?,
                    /* success */ &i64::try_from(self.success)?,
                    /* duplicate_extension */ &i64::try_from(self.duplicate_extension)?,
                    /* public_share_encode_failure */
                    &i64::try_from(self.public_share_encode_failure)?,
                    /* batch_collected */ &i64::try_from(self.batch_collected)?,
                    /* report_replayed */ &i64::try_from(self.report_replayed)?,
                    /* report_dropped */ &i64::try_from(self.report_dropped)?,
                    /* hpke_unknown_config_id */
                    &i64::try_from(self.hpke_unknown_config_id)?,
                    /* hpke_decrypt_failure */ &i64::try_from(self.hpke_decrypt_failure)?,
                    /* vdaf_prep_error */ &i64::try_from(self.vdaf_prep_error)?,
                    /* task_not_started */ &i64::try_from(self.task_not_started)?,
                    /* task_expired */ &i64::try_from(self.task_expired)?,
                    /* invalid_message */ &i64::try_from(self.invalid_message)?,
                    /* report_too_early */ &i64::try_from(self.report_too_early)?,
                    /* helper_batch_collected */
                    &i64::try_from(self.helper_batch_collected)?,
                    /* helper_report_replayed */
                    &i64::try_from(self.helper_report_replayed)?,
                    /* helper_report_dropped */
                    &i64::try_from(self.helper_report_dropped)?,
                    /* helper_hpke_unknown_config_id */
                    &i64::try_from(self.helper_hpke_unknown_config_id)?,
                    /* helper_hpke_decrypt_failure */
                    &i64::try_from(self.helper_hpke_decrypt_failure)?,
                    /* helper_vdaf_prep_error */
                    &i64::try_from(self.helper_vdaf_prep_error)?,
                    /* helper_task_not_started */
                    &i64::try_from(self.helper_task_not_started)?,
                    /* helper_task_expired */ &i64::try_from(self.helper_task_expired)?,
                    /* helper_invalid_message */
                    &i64::try_from(self.helper_invalid_message)?,
                    /* helper_report_too_early */
                    &i64::try_from(self.helper_report_too_early)?,
                ],
            )
            .await?,
        )
    }

    /// Returns true if and only if this task aggregation counter is "zero", i.e. it would not
    /// change the state of the written task aggregation counters.
    pub fn is_zero(&self) -> bool {
        self == &TaskAggregationCounter::default()
    }

    /// Increments the counter of successfully-aggregated reports.
    pub fn increment_success(&mut self) {
        self.success += 1
    }

    /// Increments the appropriate counter based on the prepare failure.
    pub fn increment_with_report_error(&mut self, error: ReportError) {
        match error {
            ReportError::BatchCollected => self.batch_collected += 1,
            ReportError::ReportReplayed => self.report_replayed += 1,
            ReportError::ReportDropped => self.report_dropped += 1,
            ReportError::HpkeUnknownConfigId => self.hpke_unknown_config_id += 1,
            ReportError::HpkeDecryptError => self.hpke_decrypt_failure += 1,
            ReportError::VdafPrepError => self.vdaf_prep_error += 1,
            ReportError::TaskNotStarted => self.task_not_started += 1,
            ReportError::TaskExpired => self.task_expired += 1,
            ReportError::InvalidMessage => self.invalid_message += 1,
            ReportError::ReportTooEarly => self.report_too_early += 1,
            _ => tracing::debug!(?error, "unexpected prepare error"),
        }
    }

    /// Increments the appropriate counter based on the helper prepare failure.
    pub fn increment_with_helper_report_error(&mut self, helper_error: ReportError) {
        match helper_error {
            ReportError::BatchCollected => self.helper_batch_collected += 1,
            ReportError::ReportReplayed => self.helper_report_replayed += 1,
            ReportError::ReportDropped => self.helper_report_dropped += 1,
            ReportError::HpkeUnknownConfigId => self.helper_hpke_unknown_config_id += 1,
            ReportError::HpkeDecryptError => self.helper_hpke_decrypt_failure += 1,
            ReportError::VdafPrepError => self.helper_vdaf_prep_error += 1,
            ReportError::TaskNotStarted => self.helper_task_not_started += 1,
            ReportError::TaskExpired => self.helper_task_expired += 1,
            ReportError::InvalidMessage => self.helper_invalid_message += 1,
            ReportError::ReportTooEarly => self.helper_report_too_early += 1,
            _ => tracing::debug!(?helper_error, "unexpected prepare error from helper"),
        }
    }
}
