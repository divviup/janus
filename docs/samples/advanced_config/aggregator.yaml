# Common configuration parameters:

database:
  # Database URL. (required)
  url: "postgres://postgres:postgres@localhost:5432/postgres"

  # Timeout for new database connections. Defaults to 60 seconds.
  connection_pool_timeout_secs: 60

  # Flag to check if the database schema version is compatible upon startup.
  # (optional, defaults to true)
  check_schema_version: true

  # Path to a PEM file with root certificates to trust for TLS database
  # connections. (optional)
  tls_trust_store_path: /path/to/file.pem

# Socket address for /healthz and /traceconfigz HTTP requests. Defaults to 127.0.0.1:9001.
health_check_listen_address: "0.0.0.0:8000"

# Logging configuration. (optional)
logging_config:
  # Flag to output structured logs. (optional)
  force_json_output: true

  # Flag to output structured logs in Google Cloud Logging's format. (optional)
  stackdriver_json_output: false

  # Configuration for the tokio-console tracing subscriber. (optional)
  tokio_console_config:
    # Enable the subscriber. (optional)
    enabled: true
    # Socket address to listen on. (optional)
    listen_address: "127.0.0.1:6669"

  # OpenTelemetry tracing configuration. This can contain an "otlp" key with a
  # map containing exporter configuration. (optional)
  open_telemetry_config:
    otlp:
      # OTLP gRPC endpoint.
      endpoint: "https://example.com"
      # gRPC metadata to send with OTLP requests. (optional)
      metadata:
        key: "value"

  # Flag to write tracing spans and events to JSON files. This is compatible
  # with Chrome's trace viewer, available at `chrome://tracing`, and
  # Perfetto, at https://ui.perfetto.dev/. (optional)
  chrome: false

# Metrics configuration. (optional)
metrics_config:
  # Metrics exporter configuration. This contains a map with single key, either
  # "prometheus" or "otlp". (optional)
  exporter:
    prometheus:
      # Address on which to listen for Prometheus metrics scrape requests. (optional)
      host: "0.0.0.0"
      # Port number for metrics server. (optional)
      port: 9464

  ##otlp:
  ##  # OTLP gRPC endpoint.
  ##  endpoint: "https://example.com/"
  ##  # gRPC metadata to send with OTLP requests. (optional)
  ##  metadata:
  ##    key: "value"

# Aggregator-specific parameters:

# Socket address for DAP requests. (required)
listen_address: "0.0.0.0:8080"

# How to serve the Janus aggregator API. If not set, Janus aggregator API is not served. (optional)
aggregator_api:
  # Serve the aggregator API on an address and port that is separate from the DAP API. This is
  # mutually exclusive with path_prefix.
  listen_address: "0.0.0.0:8081"

  # Alternatively, serve the aggregator API on the same address as the DAP API, but on a separate
  # path denoted by `path_prefix`. This is mutually exclusive with listen_address.
  # path_prefix: "aggregator-api"

  # Resource location at which the DAP service managed by this
  # aggregator api can be found on the public internet. Required.
  public_dap_url: "https://dap.test"

# Maximum number of uploaded reports per batching transaction. (required)
max_upload_batch_size: 100

# Maximum delay before writing a batch of uploaded reports. (required)
max_upload_batch_write_delay_ms: 250

# Number of sharded database records per batch aggregation. Must not be greater
# than the equivalent setting in the collection job driver. (required)
batch_aggregation_shard_count: 32

# Configuration for the taskprov extension. If enabled, this changes the behavior of the
# aggregator as described in draft-wang-ppm-dap-taskprov. (optional)
taskprov_config:
  # Whether to enable the taskprov extension. Defaults to false.
  enabled: false

# Configuration for garbage collection. If omitted, old data is never deleted. (optional)
garbage_collection:
  # How frequently to collect garbage, in seconds.
  gc_frequency_s: 60

  # The maximum number of client reports, per task, to delete in a single run of the garbage
  # collector.
  report_limit: 5000

  # The maximum number of aggregation jobs (& related artifacts), per task, to delete in a single
  # run of the garbage collector.
  aggregation_limit: 500

  # The maximum number of collection jobs (& related artifacts), per task, to delete in a single run
  # of the garbage collector.
  collection_limit: 50
