# Common configuration parameters:

database:
  # Database URL. (required)
  url: "postgres://postgres:postgres@localhost:5432/postgres"

  # Timeout for new database connections. Defaults to 60 seconds.
  connection_pool_timeouts_s: 60

  # Maximum number of database connections. Defaults to CPUs * 4.
  connection_pool_max_size: 8

  # Flag to check if the database schema version is compatible upon startup.
  # (optional, defaults to true)
  check_schema_version: true

  # Path to a PEM file with root certificates to trust for TLS database
  # connections. If present, TLS may be used depending on the "sslmode"
  # connection string parameter, and the server's support for TLS. If absent,
  # TLS will never be used. (optional)
  tls_trust_store_path: /path/to/file.pem

# The maximum number of times a transaction can be retried. This is intended to
# guard against bugs that induce infinite retries. It should be set to a
# reasonably high limit to prevent legitimate work from being cancelled.
max_transaction_retries: 1000

# Socket address for /healthz and /traceconfigz HTTP requests. Defaults to 127.0.0.1:9001.
health_check_listen_address: "0.0.0.0:8000"

# Logging configuration. (optional)
logging_config:
  # Flag to output structured logs. (optional)
  force_json_output: true

  # Flag to output structured logs in Google Cloud Logging's format. (optional)
  stackdriver_json_output: false

  # Configuration for the tokio-console tracing subscriber. (optional)
  tokio_console_config:
    # Enable the subscriber. (optional)
    enabled: true
    # Socket address to listen on. (optional)
    listen_address: "127.0.0.1:6669"

  # OpenTelemetry tracing configuration. This can contain an "otlp" key with a
  # map containing exporter configuration. (optional)
  open_telemetry_config:
    otlp:
      # OTLP gRPC endpoint.
      endpoint: "https://example.com"

  # Flag to write tracing spans and events to JSON files. This is compatible
  # with Chrome's trace viewer, available at `chrome://tracing`, and
  # Perfetto, at https://ui.perfetto.dev/. (optional)
  chrome: false

# Metrics configuration. (optional)
metrics_config:
  # Metrics exporter configuration. This contains a map with single key, either
  # "prometheus" or "otlp". (optional)
  exporter:
    prometheus:
      # Address on which to listen for Prometheus metrics scrape requests. (optional)
      host: "0.0.0.0"
      # Port number for metrics server. (optional)
      port: 9464

  ##otlp:
  ##  # OTLP gRPC endpoint.
  ##  endpoint: "https://example.com/"

  # Configuration for Tokio runtime metrics. (optional)
  tokio:
    # Enable exporting metrics from the Tokio runtime. If this is true, the
    # binary must have been compiled with the flag `--cfg tokio_unstable`.
    # (optional)
    enabled: false
    # Enable a histogram of task poll times. This introduces some additional
    # overhead. (optional)
    enable_poll_time_histogram: false
    # Selects whether to use a `"linear"` scale or a logarithmic (`"log"`) scale
    # for the poll time histogram. Defaults to `"linear"`. (optional)
    poll_time_histogram_scale: linear
    # Sets the poll time histogram's resolution. Defaults to 100 microseconds.
    # (optional)
    poll_time_histogram_resolution_us: 100
    # Sets the number of buckets in the poll time histogram. Defaults to 10.
    # (optional)
    poll_time_histogram_buckets: 10

# Aggregator-specific parameters:

# Socket address for DAP requests. (required)
listen_address: "0.0.0.0:8080"

# How to serve the Janus aggregator API. If not set, Janus aggregator API is not served. (optional)
aggregator_api:
  # Serve the aggregator API on an address and port that is separate from the DAP API. This is
  # mutually exclusive with path_prefix.
  listen_address: "0.0.0.0:8081"

  # Alternatively, serve the aggregator API on the same address as the DAP API, but on a separate
  # path denoted by `path_prefix`. This is mutually exclusive with listen_address.
  # path_prefix: "aggregator-api"

  # Resource location at which the DAP service managed by this
  # aggregator api can be found on the public internet. Required.
  public_dap_url: "https://dap.test"

# Maximum number of uploaded reports per batching transaction. (required)
max_upload_batch_size: 100

# Maximum delay before writing a batch of uploaded reports. (required)
max_upload_batch_write_delay_ms: 250

# Number of sharded database records per batch aggregation. Must not be greater
# than the equivalent setting in the collection job driver. (required)
batch_aggregation_shard_count: 32

# Defines the number of shards to break report & aggregation metric counters into. Increasing this
# value will reduce the amount of database contention during report uploads & aggregations, while
# increasing the cost of getting task metrics. (optional, default: 32)
task_counter_shard_count: 32

# Configuration for the taskprov extension. If enabled, this changes the behavior of the
# aggregator as described in draft-wang-ppm-dap-taskprov. (optional)
taskprov_config:
  # Whether to enable the taskprov extension. Defaults to false.
  enabled: false

# Where forbidden mutations should be logged. In DAP, it's permitted to retry HTTP requests like
# aggregation job creation/initialization, but it's not legal to mutate the resource, for example by
# changing which reports are included in an aggregation job. Janus always rejects such forbidden
# mutations. If this option is enabled, then it will log the hash of the request that created the
# resource, the hash of the incoming request that attempted to mutate it, and the list of report
# metadatas in both, into a file created under the path provided here.
#
# Janus does not unconditionally log this information because retrieving the report metadata for the
# existing request is expensive and the resulting event is large.
#
# This option is not stable and not subject to Janus' typical API and configuration stability
# promises.
#
# Optional; the default is not to log events.
log_forbidden_mutations: "/tmp/event-storage"

# Configuration for garbage collection. If omitted, old data is never deleted. (optional)
garbage_collection:
  # How frequently to collect garbage, in seconds.
  gc_frequency_s: 60

  # The maximum number of client reports, per task, to delete in a single run of the garbage
  # collector.
  report_limit: 5000

  # The maximum number of aggregation jobs (& related artifacts), per task, to delete in a single
  # run of the garbage collector.
  aggregation_limit: 500

  # The maximum number of collection jobs (& related artifacts), per task, to delete in a single run
  # of the garbage collector.
  collection_limit: 50

  # The maximum number of tasks to process together for GC in a single database
  # transaction. (optional)
  tasks_per_tx: 1

  # The maximum number of concurrent database transactions to open at once while
  # processing GC. Leaving this unset means there is no maximum. (optional)
  concurrent_tx_limit: null

# Configuration for key rotator. Allows running the key rotator as part of the aggregator process.
# If omitted, you should run the key rotator as a separate cronjob.
key_rotator:
  # How frequently to run the key rotator, in seconds. Required.
  frequency_s: 3600

  # Rotation policy for global HPKE keys.
  hpke:
    # How long keys remains pending before they're promoted to active. Should
    # be greater than the global HPKE keypair cache refresh rate. Defaults to
    # 1 hour.
    pending_duration_s: 3600

    # The TTL of keys. Defaults to 4 weeks.
    active_duration_s: 7257600

    # How long keys can be expired before being deleted. Should be greater than
    # how long clients cache HPKE keys. Defaults to 1 week.
    expired_duration_s: 604800

    # The set of keys to manage, identified by ciphersuite. At least one is
    # required. Each entry represents a key with a particular ciphersuite.
    ciphersuites:
      # Defaults to a key with these algorithms.
      - kem_id: X25519HkdfSha256
        kdf_id: HkdfSha256
        aead_id: Aes128Gcm

# Defines how often to refresh the global HPKE configs cache, in milliseconds.
# This affects how often an aggregator becomes aware of key state changes.
# (optional, defaults to 30 minutes)
global_hpke_configs_refresh_interval: 1800000

# Defines how long to cache tasks for, in seconds. This affects how soon the
# aggregator becomes aware of task parameter changes. (optional, defaults to 10
# minutes)
task_cache_ttl_s: 600

# Defines how many tasks can be cached. This affects how much memory the
# aggregator might use to store cached tasks. (optional)
task_cache_capacity: 10000
